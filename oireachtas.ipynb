{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jmcinern/Oireachtas_RAG/blob/main/oireachtas.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9RQoezxTievr"
      },
      "source": [
        "# Oireachtas"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ITEq4-zNmrIy"
      },
      "source": [
        "**Project by Joseph McInerney.**\n",
        "- This project aims to allow Irish citizens easily get relavant primary source political information to allow for minimal framing to let the user come to their own conclusions.\n",
        "- The aim of the project is to allow functionality whereby:\n",
        "  - Oireactas speeches are collected and stored along with their TD.\n",
        "  - This is stored in a vector data base mapping speeches in semantic space (speeches covering similar topics are closer together).\n",
        "  - A Large Language Model (LLM) is used to query this database responding with summaries of TD's positions on issues with direct quotes from the Oireactais data base.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BQZ-oMWhliZl"
      },
      "source": [
        "# Requirements"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MW_SBm7QuCyk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ffcebe20-44d4-4215-f5f3-d4ef24c6631d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/67.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.1/76.1 MB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m491.4/491.4 kB\u001b[0m \u001b[31m31.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.3/18.3 MB\u001b[0m \u001b[31m95.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m66.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.9/94.9 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.1/54.1 MB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m322.9/322.9 kB\u001b[0m \u001b[31m21.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m661.3/661.3 kB\u001b[0m \u001b[31m36.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m69.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m284.2/284.2 kB\u001b[0m \u001b[31m20.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.6/193.6 kB\u001b[0m \u001b[31m15.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m72.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.6/101.6 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.5/143.5 kB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.0/16.0 MB\u001b[0m \u001b[31m90.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.9/55.9 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.0/92.0 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.4/44.4 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m83.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.5/11.5 MB\u001b[0m \u001b[31m98.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.5/71.5 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m91.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m83.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m52.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m61.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.5/62.5 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.8/194.8 kB\u001b[0m \u001b[31m16.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m459.8/459.8 kB\u001b[0m \u001b[31m29.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.0/4.0 MB\u001b[0m \u001b[31m70.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m454.8/454.8 kB\u001b[0m \u001b[31m29.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2025.3.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install -qU bitsandbytes datasets accelerate loralib chromadb peft gradio thefuzz[speedup] langchain openai langchain_community"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IROCurCnHFtA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f26e6198-3b8a-463f-e95d-0c9869cfde8c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AclDneqtlgKm"
      },
      "outputs": [],
      "source": [
        "import requests # for getting web data\n",
        "import xml.etree.ElementTree as ET # for easy XML parsing\n",
        "from datetime import datetime, timedelta # for knowing debate url search window\n",
        "from tqdm import tqdm # for tracking progress\n",
        "import concurrent.futures # for parallel processing\n",
        "import chromadb # vector database\n",
        "from google.colab import drive # for storing vector dbimport pickle\n",
        "import os\n",
        "from sentence_transformers import SentenceTransformer # embedding\n",
        "import transformers # for hugging face model\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import pandas as pd\n",
        "from thefuzz import process\n",
        "import os # user-input API key\n",
        "import openai # gpt\n",
        "import langchain # for promtp template and few-shot\n",
        "# allows for roles and system messages unlike simple PromptTemplate and integrates with few-shot\n",
        "from langchain.prompts import (FewShotChatMessagePromptTemplate, ChatPromptTemplate, PromptTemplate)\n",
        "from langchain.schema import HumanMessage # seperate examples cleanly\n",
        "from getpass import getpass # for API key\n",
        "from langchain.chat_models import ChatOpenAI # to initialise model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VFxdJPH_NZcA"
      },
      "outputs": [],
      "source": [
        "project_fpath = r\"/content/drive/MyDrive/Oireachtas_RAG/\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I3vJW3pOhjg2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 339
        },
        "outputId": "2a8b04b9-9f69-47a7-e712-0b13847faa6c"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "Interrupted by user",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-138cddfcd286>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# getOpenAI key\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'OPENAI_KEY'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetpass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Enter your OpenAI API key: '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mopenai\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapi_key\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetenv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'OPENAI_KEY'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mgetpass\u001b[0;34m(self, prompt, stream)\u001b[0m\n\u001b[1;32m   1157\u001b[0m                 \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1158\u001b[0m             )\n\u001b[0;32m-> 1159\u001b[0;31m         return self._input_request(\n\u001b[0m\u001b[1;32m   1160\u001b[0m             \u001b[0mprompt\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1161\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"shell\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m   1217\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1218\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1219\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Interrupted by user\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1220\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1221\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid Message:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
          ]
        }
      ],
      "source": [
        "# getOpenAI key\n",
        "os.environ['OPENAI_KEY'] = getpass('Enter your OpenAI API key: ')\n",
        "openai.api_key = os.getenv('OPENAI_KEY')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AOjyoLYaievt"
      },
      "source": [
        "# Data Collection"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OSlbu7E8iiZN"
      },
      "source": [
        "**Code that accesses Oireachtas debate records and parses XML to store info.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RIZ8of2dievt"
      },
      "source": [
        "## Debate URLs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ziEWYym6loki"
      },
      "source": [
        "\n",
        "\n",
        "*   **Genereate list of URLs pertaining to oireachtas debates given a time frame**\n",
        "*   **Example: https://data.oireachtas.ie/akn/ie/debateRecord/dail/2013-05-03/debate/mul@/main.xml**\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LaNriSwSievu"
      },
      "outputs": [],
      "source": [
        "# Define the namespace for XML parsing\n",
        "NS = {\"akn\": \"http://docs.oasis-open.org/legaldocml/ns/akn/3.0/CSD13\"}\n",
        "\n",
        "# Function to generate URLs, default: for the last month\n",
        "def get_XML_urls(number_of_days=30):\n",
        "    urls = []\n",
        "    today = datetime.today()\n",
        "    start_date = today - timedelta(number_of_days)  # Last 30 days\n",
        "\n",
        "    for i in tqdm(range(number_of_days)):\n",
        "        # URL format: https://data.oireachtas.ie/akn/ie/debateRecord/dail/YYYY-MM-DD/debate/mul@/main.xml\n",
        "        date_str = (start_date + timedelta(days=i)).strftime(\"%Y-%m-%d\")\n",
        "        url = f\"https://data.oireachtas.ie/akn/ie/debateRecord/dail/{date_str}/debate/mul@/main.xml\"\n",
        "        urls.append(url)\n",
        "\n",
        "    return urls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DpqvcX4ylTmZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dc308e46-378d-4933-e23f-a387f0004c07"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 3650/3650 [00:00<00:00, 255869.93it/s]\n"
          ]
        }
      ],
      "source": [
        "# Generate URLs for the last set amount of days\n",
        "number_of_days = 3650 # ~10 years\n",
        "debate_urls = get_XML_urls(number_of_days)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gQBkOoEtq02D"
      },
      "source": [
        "## Fetching Speeches"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fnH5tqeDmVBM"
      },
      "source": [
        "- Given each URL there may be multiple speeches by multiple TDs.\n",
        "- So store each speech with the relevant TD as well."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jb85k3yK5gbi"
      },
      "source": [
        "Extracts the debate language:\n",
        "It looks for the <FRBRlanguage> element (using the same namespace as other\n",
        "\n",
        "\n",
        "* elements) and reads its “language” attribute. In the example XML, the language value is \"eng\". The code then maps that to a two‐letter code (e.g. \"eng\" becomes \"en\" and you could map others such as \"gle\" to \"ga\").\n",
        "\n",
        "* Counts the total number of words in the debate:\n",
        "For each speech extracted, it joins all paragraph texts and then splits the text by spaces to count words. All these counts are summed to give the overall word count.\n",
        "\n",
        "* Returns a dictionary with the debate language, word count, and the list of speeches:\n",
        "\n",
        "* Each speech is stored as a dictionary with keys \"speaker\" and \"text\"."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "95QapCl6lS9W"
      },
      "outputs": [],
      "source": [
        "SPEAKERS = set() # set to match user query and data base speaker name ID\n",
        "def fetch_and_extract_speeches(url):\n",
        "    try:\n",
        "        response = requests.get(url)\n",
        "\n",
        "        # debate found\n",
        "        if response.status_code == 200:\n",
        "\n",
        "            root = ET.fromstring(response.content)\n",
        "            speeches = []\n",
        "\n",
        "            # Extract all <speech> elements\n",
        "            for speech in root.findall(\".//akn:speech\", namespaces=NS):\n",
        "\n",
        "                speaker = speech.get(\"by\", \"Unknown Speaker\").strip(\"#\")  # Extract speaker ID\n",
        "                paragraphs = [p.text.strip() for p in speech.findall(\".//akn:p\", namespaces=NS) if p.text]\n",
        "\n",
        "                if paragraphs:\n",
        "                    full_text = \" \".join(paragraphs)\n",
        "                    speeches.append({\"speaker\": speaker, \"text\": full_text, \"url\": url})\n",
        "                    SPEAKERS.add(speaker)\n",
        "\n",
        "            return speeches\n",
        "        else:\n",
        "            return None\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing {url}: {e}\")\n",
        "        return None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rNfZa29Jp5EO"
      },
      "source": [
        "- Leverage parallel processing to fetch urls.\n",
        "- ~20 seconds for 10 years worth of data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wAvUqR-vo6Bp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ca6aae5e-6312-4756-fd0f-06be5d24a7c1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "3650it [00:58, 62.53it/s] \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'speaker': 'SeanOFearghaillFF', 'text': \"I have the unenviable task of standing in for the inimitable Deputy O'Dea. This question seeks to explore with the Tánaiste what plans, if any, she has to extend access to social welfare benefits to the self-employed. The question is posed against the background of all of us in this House wishing to see the indigenous sector develop. We see access to welfare benefits as part of that necessary change.\", 'url': 'https://data.oireachtas.ie/akn/ie/debateRecord/dail/2015-05-06/debate/mul@/main.xml'}\n",
            "Number of speeches 375685 from the last 3650 days\n"
          ]
        }
      ],
      "source": [
        "import concurrent.futures\n",
        "from tqdm import tqdm\n",
        "\n",
        "def fetch_speeches_parallel(urls):\n",
        "    all_speeches = []\n",
        "    with concurrent.futures.ThreadPoolExecutor(max_workers=100) as executor:\n",
        "        results = list(tqdm(executor.map(fetch_and_extract_speeches, urls)))\n",
        "        for result in results:\n",
        "            if result is not None:\n",
        "                all_speeches.extend(result)\n",
        "    return all_speeches\n",
        "\n",
        "# Usage:\n",
        "all_speeches = fetch_speeches_parallel(debate_urls)\n",
        "print(all_speeches[0])\n",
        "print(f\"Number of speeches {len(all_speeches)} from the last {number_of_days} days\")\n",
        "\n",
        "\n",
        "# write SPEAKERS to txt file\n",
        "with open(project_fpath+\"speakers.txt\", \"w\") as f:\n",
        "    for speaker in SPEAKERS:\n",
        "        f.write(f\"{speaker}\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jaopG7pKq-cI"
      },
      "source": [
        "# Vector Database"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pbwojCyBievv"
      },
      "source": [
        "## Embedding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0blNCfGbievv"
      },
      "source": [
        "- Chroma embeds with all-MiniLM-L6-v2, 384 dim embedding trained with cosine.\n",
        "- Euclidean (l2) best for RAG-Chroma: https://medium.com/@stepkurniawan/comparing-similarity-searches-distance-metrics-in-vector-stores-rag-model-f0b3f7532d6f"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BUAC5OUGievv",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "outputId": "10bcb4f0-eca7-4082-e250-a57e65a182f7"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-34501587fb74>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mcollection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mchroma_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_or_create_collection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"oireachtas_debates\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0;32mif\u001b[0m \u001b[0mcollection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m   \u001b[0;31m# ─── 1) load your embedding model ───────────────────────────────────────────────\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m   \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSentenceTransformer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"all-MiniLM-L6-v2\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"cuda\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/chromadb/api/models/Collection.py\u001b[0m in \u001b[0;36mcount\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m         \"\"\"\n\u001b[0;32m---> 39\u001b[0;31m         return self._client._count(\n\u001b[0m\u001b[1;32m     40\u001b[0m             \u001b[0mcollection_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m             \u001b[0mtenant\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtenant\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/chromadb/api/rust.py\u001b[0m in \u001b[0;36m_count\u001b[0;34m(self, collection_id, tenant, database)\u001b[0m\n\u001b[1;32m    323\u001b[0m         \u001b[0mdatabase\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDEFAULT_DATABASE\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    324\u001b[0m     ) -> int:\n\u001b[0;32m--> 325\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbindings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcollection_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtenant\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdatabase\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    326\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    327\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0moverride\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "import uuid\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from chromadb.utils.batch_utils import create_batches\n",
        "\n",
        "# ─── 0) Set up your Chroma client ───────────────────────────────────────────────\n",
        "# (PersistentClient writes to disk; for ephemeral/in-memory use chromadb.Client())\n",
        "chroma_client = chromadb.PersistentClient(\n",
        "    path=project_fpath + \"/debate_db\"    # wherever you want the on-disk store\n",
        ")\n",
        "collection = chroma_client.get_or_create_collection(\"oireachtas_debates\")\n",
        "\n",
        "if collection.count()==0:\n",
        "  # ─── 1) load your embedding model ───────────────────────────────────────────────\n",
        "  model = SentenceTransformer(\"all-MiniLM-L6-v2\", device=\"cuda\")\n",
        "\n",
        "  # ─── 2) build your parallel lists ───────────────────────────────────────────────\n",
        "  ids       = [str(uuid.uuid4()) for _ in all_speeches]\n",
        "  texts     = [s[\"text\"] for s in all_speeches]\n",
        "  metadatas = [\n",
        "      {\"speaker\": s[\"speaker\"], \"url\": s[\"url\"], \"text\": s[\"text\"]}\n",
        "      for s in all_speeches\n",
        "  ]\n",
        "\n",
        "  # ─── 3) encode everything at once (or in large chunks) ─────────────────────────\n",
        "  embeddings = model.encode(texts, batch_size=512, show_progress_bar=True)\n",
        "  embeddings = embeddings.tolist()\n",
        "\n",
        "  # model.encode returns numpy erray but creat_batches() expects list\n",
        "  # ─── 4) batch-slice to avoid SQLite param limits ────────────────────────────────\n",
        "  batches = create_batches(\n",
        "      api=chroma_client,\n",
        "      ids=ids,\n",
        "      embeddings=embeddings,\n",
        "      metadatas=metadatas,\n",
        "      documents=texts,\n",
        "  )\n",
        "\n",
        "  # ─── 5) fire each super-chunk into Chroma ──────────────────────────────────────\n",
        "  collection = chroma_client.get_or_create_collection(\"oireachtas_debates\")\n",
        "  for ids_b, embs_b, metas_b, docs_b in tqdm(batches):\n",
        "      print(f\"Adding batch of {len(ids_b)} docs …\")\n",
        "      collection.add(\n",
        "          ids=ids_b,\n",
        "          embeddings=embs_b,\n",
        "          metadatas=metas_b,\n",
        "          documents=docs_b,\n",
        "      )"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chroma_client = chromadb.PersistentClient(\n",
        "    path=project_fpath + \"/debate_db\"\n",
        ")\n",
        "collection = chroma_client.get_collection(\"oireachtas_debates\")"
      ],
      "metadata": {
        "id": "n7Cn3NRtpcA9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ROW8Ro73_WaL"
      },
      "source": [
        "## RAG"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T_eZyhg7a7Vt"
      },
      "source": [
        "### **Retrieval**\n",
        "\n",
        "\n",
        "*   Fetch top k utterences by speaker on topic using vector DB.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4HCZlgtIievw"
      },
      "outputs": [],
      "source": [
        "def search_speaker_position(speaker_name, topic, num_results=5):\n",
        "    # Use ChromaDB's query functionality with `where` clause for speaker\n",
        "    print(\"Looking for relevant utterences\")\n",
        "    results = collection.query(\n",
        "        query_texts=[topic],\n",
        "        n_results=num_results,\n",
        "        where={\"speaker\": speaker_name},\n",
        "        include=[\"metadatas\"]\n",
        "    )\n",
        "    print(\"Done looking for relevant utterences\")\n",
        "    print(results)\n",
        "    # Check if any results were found\n",
        "    if not results['metadatas'][0]:  # ChromaDB returns a list of lists\n",
        "        return f\"No speeches found for {speaker_name}  talking about {topic} in the dataset.\"\n",
        "\n",
        "    # Extract and format the results\n",
        "    output = f\"\\n### {speaker_name}'s Position on '{topic}':\\n\"\n",
        "    for i, metadata in enumerate(results['metadatas'][0]):\n",
        "        output += f\"\\n **Quote {i+1} (debate url: {metadata['url']}):** {metadata['text'][:500]}...\\n\"\n",
        "\n",
        "    return output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eu01sOXXiU12"
      },
      "source": [
        "#### Few-shot prompt template - Langchain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aPEQZTtC8buN"
      },
      "outputs": [],
      "source": [
        "examples=[\n",
        "    #1\n",
        "    {\n",
        "  \"question\": \"Summarize MicheálMartin's position on healthcare, weaving in short quotes placed within quotation marks from the reference material.\\n\\n### MicheálMartin's Position on 'healthcare':\\n\\n**Quote 1 (debate url: https://data.oireachtas.ie/akn/ie/debateRecord/dail/2011-05-03/debate/mul@/main.xml, year: 2011):** \\\"Healthcare must be a right, not a privilege, ensuring no citizen is denied essential treatment because of cost or location.\\\"\\n\\n**Quote 2 (debate url: https://data.oireachtas.ie/akn/ie/debateRecord/dail/2012-05-03/debate/mul@/main.xml, year: 2012):** \\\"Investment in modern hospital infrastructure is a critical pillar for equitable access to services across Ireland.\\\"\\n\\n**Quote 3 (debate url: https://data.oireachtas.ie/akn/ie/debateRecord/dail/2013-05-03/debate/mul@/main.xml, year: 2013):** \\\"Primary care should serve as the bedrock of our health system, treating issues early and reducing reliance on emergency departments.\\\"\\n\\n**Quote 4 (debate url: https://data.oireachtas.ie/akn/ie/debateRecord/dail/2014-05-03/debate/mul@/main.xml, year: 2014):** \\\"Strengthening Ireland's digital economy is key to attracting global investment and future-proofing our industries.\\\" (irrelevant)\\n\\n**Quote 5 (debate url: https://data.oireachtas.ie/akn/ie/debateRecord/dail/2015-05-03/debate/mul@/main.xml, year: 2015):** \\\"Climate action must be deeply integrated into all areas of policy, including education, housing, and agriculture.\\\" (irrelevant)\",\n",
        "\n",
        "  \"answer\": \"Micheál Martin emphasizes that 'healthcare must be a right, not a privilege' (2011, https://data.oireachtas.ie/akn/ie/debateRecord/dail/2011-05-03/debate/mul@/main.xml). He highlights 'investment in modern hospital infrastructure' as essential (2012, https://data.oireachtas.ie/akn/ie/debateRecord/dail/2012-05-03/debate/mul@/main.xml) and asserts that 'primary care should serve as the bedrock of our health system' (2013, https://data.oireachtas.ie/akn/ie/debateRecord/dail/2013-05-03/debate/mul@/main.xml).\"\n",
        "    },\n",
        "    #2\n",
        "    {\n",
        "  \"question\": \"Summarize LeoVaradkar's statements on economic recovery, interweaving short quotations from the reference text.\\n\\n### LeoVaradkar's Position on 'economic recovery':\\n\\n**Quote 1 (debate url: https://data.oireachtas.ie/akn/ie/debateRecord/dail/2016-06-04/debate/mul@/main.xml, year: 2016):** \\\"Recovery must be fair and inclusive, lifting every household that bore the brunt of austerity.\\\"\\n\\n**Quote 2 (debate url: https://data.oireachtas.ie/akn/ie/debateRecord/dail/2017-06-04/debate/mul@/main.xml, year: 2017):** \\\"Direct supports for small businesses will drive sustainable growth across Ireland.\\\"\\n\\n**Quote 3 (debate url: https://data.oireachtas.ie/akn/ie/debateRecord/dail/2018-06-04/debate/mul@/main.xml, year: 2018):** \\\"Promoting cycling in cities is central to tackling traffic congestion.\\\" (irrelevant)\\n\\n**Quote 4 (debate url: https://data.oireachtas.ie/akn/ie/debateRecord/dail/2019-06-04/debate/mul@/main.xml, year: 2019):** \\\"Fiscal prudence ensures we do not burden future generations with today's mistakes.\\\"\\n\\n**Quote 5 (debate url: https://data.oireachtas.ie/akn/ie/debateRecord/dail/2020-06-04/debate/mul@/main.xml, year: 2020):** \\\"Strong ties with the European Union strengthen Ireland’s economic resilience.\\\" (irrelevant)\",\n",
        "\n",
        "  \"answer\": \"Leo Varadkar insists that 'recovery must be fair and inclusive' (2016, https://data.oireachtas.ie/akn/ie/debateRecord/dail/2016-06-04/debate/mul@/main.xml) and stresses that 'direct supports for small businesses will drive sustainable growth' (2017, https://data.oireachtas.ie/akn/ie/debateRecord/dail/2017-06-04/debate/mul@/main.xml). He further notes that 'fiscal prudence ensures we do not burden future generations' (2019, https://data.oireachtas.ie/akn/ie/debateRecord/dail/2019-06-04/debate/mul@/main.xml).\"\n",
        "    },\n",
        "    #3\n",
        "    {\n",
        "  \"question\": \"Summarize MaryLouMcDonald's comments on housing, blending brief quotes from the reference material into the summary.\\n\\n### MaryLouMcDonald's Position on 'housing':\\n\\n**Quote 1 (debate url: https://data.oireachtas.ie/akn/ie/debateRecord/dail/2015-07-10/debate/mul@/main.xml, year: 2015):** \\\"The housing crisis is not accidental; it is the direct result of policy choices that neglected social needs.\\\"\\n\\n**Quote 2 (debate url: https://data.oireachtas.ie/akn/ie/debateRecord/dail/2016-07-10/debate/mul@/main.xml, year: 2016):** \\\"A massive programme of public housing construction is essential to meet demand.\\\"\\n\\n**Quote 3 (debate url: https://data.oireachtas.ie/akn/ie/debateRecord/dail/2017-07-10/debate/mul@/main.xml, year: 2017):** \\\"Strengthening the rural broadband network will unlock opportunities across Ireland.\\\" (irrelevant)\\n\\n**Quote 4 (debate url: https://data.oireachtas.ie/akn/ie/debateRecord/dail/2018-07-10/debate/mul@/main.xml, year: 2018):** \\\"Rents are out of control and the dream of homeownership is slipping away for an entire generation.\\\"\\n\\n**Quote 5 (debate url: https://data.oireachtas.ie/akn/ie/debateRecord/dail/2019-07-10/debate/mul@/main.xml, year: 2019):** \\\"Political will is needed to solve this, not more handouts to developers.\\\"\",\n",
        "\n",
        "  \"answer\": \"Mary Lou McDonald argues that 'the housing crisis is not accidental' (2015, https://data.oireachtas.ie/akn/ie/debateRecord/dail/2015-07-10/debate/mul@/main.xml). She demands that 'a massive programme of public housing construction is essential' (2016, https://data.oireachtas.ie/akn/ie/debateRecord/dail/2016-07-10/debate/mul@/main.xml), warns that 'rents are out of control' (2018, https://data.oireachtas.ie/akn/ie/debateRecord/dail/2018-07-10/debate/mul@/main.xml), and stresses that 'political will is needed to solve this' (2019, https://data.oireachtas.ie/akn/ie/debateRecord/dail/2019-07-10/debate/mul@/main.xml).\"\n",
        "    },\n",
        "    #4\n",
        "    {\n",
        "  \"question\": \"Summarize EamonRyan's views on climate policy, incorporating brief excerpts from the reference material.\\n\\n### EamonRyan's Position on 'climate policy':\\n\\n**Quote 1 (debate url: https://data.oireachtas.ie/akn/ie/debateRecord/dail/2013-04-15/debate/mul@/main.xml, year: 2013):** \\\"Climate action is no longer optional; it is the defining issue for this generation.\\\"\\n\\n**Quote 2 (debate url: https://data.oireachtas.ie/akn/ie/debateRecord/dail/2014-04-15/debate/mul@/main.xml, year: 2014):** \\\"Renewable energy must form the cornerstone of any credible climate strategy.\\\"\\n\\n**Quote 3 (debate url: https://data.oireachtas.ie/akn/ie/debateRecord/dail/2015-04-15/debate/mul@/main.xml, year: 2015):** \\\"Strengthening community policing initiatives enhances public safety.\\\" (irrelevant)\\n\\n**Quote 4 (debate url: https://data.oireachtas.ie/akn/ie/debateRecord/dail/2016-04-15/debate/mul@/main.xml, year: 2016):** \\\"Protecting Ireland’s biodiversity is vital for environmental and economic sustainability.\\\"\\n\\n**Quote 5 (debate url: https://data.oireachtas.ie/akn/ie/debateRecord/dail/2017-04-15/debate/mul@/main.xml, year: 2017):** \\\"A just transition must ensure that no worker or community is left behind.\\\"\",\n",
        "\n",
        "  \"answer\": \"Eamon Ryan declares that 'climate action is no longer optional' (2013, https://data.oireachtas.ie/akn/ie/debateRecord/dail/2013-04-15/debate/mul@/main.xml) and insists that 'renewable energy must form the cornerstone' (2014, https://data.oireachtas.ie/akn/ie/debateRecord/dail/2014-04-15/debate/mul@/main.xml). He emphasizes that 'protecting Ireland’s biodiversity is vital' (2016, https://data.oireachtas.ie/akn/ie/debateRecord/dail/2016-04-15/debate/mul@/main.xml) and concludes that 'a just transition must ensure no community is left behind' (2017, https://data.oireachtas.ie/akn/ie/debateRecord/dail/2017-04-15/debate/mul@/main.xml).\"\n",
        "    },\n",
        "    #5\n",
        "    {\n",
        "  \"question\": \"Summarize MicheálMartin's position on education reform, using short quotes from the reference material.\\n\\n### MicheálMartin's Position on 'education reform':\\n\\n**Quote 1 (debate url: https://data.oireachtas.ie/akn/ie/debateRecord/dail/2012-11-25/debate/mul@/main.xml, year: 2012):** \\\"Every child, regardless of background, deserves a world-class education.\\\"\\n\\n**Quote 2 (debate url: https://data.oireachtas.ie/akn/ie/debateRecord/dail/2013-11-25/debate/mul@/main.xml, year: 2013):** \\\"Investment in schools and teachers is critical for equality of opportunity.\\\"\\n\\n**Quote 3 (debate url: https://data.oireachtas.ie/akn/ie/debateRecord/dail/2014-11-25/debate/mul@/main.xml, year: 2014):** \\\"Robust transport links are vital to regional economic growth.\\\" (irrelevant)\\n\\n**Quote 4 (debate url: https://data.oireachtas.ie/akn/ie/debateRecord/dail/2015-11-25/debate/mul@/main.xml, year: 2015):** \\\"Education must not end at school — lifelong learning must be embraced.\\\"\\n\\n**Quote 5 (debate url: https://data.oireachtas.ie/akn/ie/debateRecord/dail/2016-11-25/debate/mul@/main.xml, year: 2016):** \\\"Access to higher education must not be determined by parental wealth.\\\"\",\n",
        "\n",
        "  \"answer\": \"Micheál Martin affirms that 'every child deserves a world-class education' (2012, https://data.oireachtas.ie/akn/ie/debateRecord/dail/2012-11-25/debate/mul@/main.xml). He argues that 'investment in schools and teachers is critical' (2013, https://data.oireachtas.ie/akn/ie/debateRecord/dail/2013-11-25/debate/mul@/main.xml), calls for embracing 'lifelong learning' (2015, https://data.oireachtas.ie/akn/ie/debateRecord/dail/2015-11-25/debate/mul@/main.xml), and stresses that 'access to higher education must not be determined by parental wealth' (2016, https://data.oireachtas.ie/akn/ie/debateRecord/dail/2016-11-25/debate/mul@/main.xml).\"\n",
        "    }\n",
        "]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "plmWv95TidYL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d8286364-ed04-4ffc-f224-9c8595b870af"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-15-37dbdc871c25>:15: LangChainDeprecationWarning: The class `ChatOpenAI` was deprecated in LangChain 0.0.10 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-openai package and should be used instead. To use it run `pip install -U :class:`~langchain-openai` and import as `from :class:`~langchain_openai import ChatOpenAI``.\n",
            "  gpt_mini = ChatOpenAI(\n"
          ]
        }
      ],
      "source": [
        "example_prompt = ChatPromptTemplate.from_messages(\n",
        "[('human', '{question}?'), ('ai', '{answer}\\n')]\n",
        ")\n",
        "few_shot_prompt = FewShotChatMessagePromptTemplate(\n",
        "    examples=examples,\n",
        "    example_prompt=example_prompt,\n",
        ")\n",
        "full_prompt = ChatPromptTemplate.from_messages([\n",
        "  (\"system\", \"you are an Irish parliament chatbot, users will ask you questions about polititian's opinions on topics, you will provide summaries of their positions with reference to quotes that you will be provided as reference. You will not make anything up, you will not add quotes that are not relevant to the topic. These quotes have corresponding URLs, you will cite the quote using the URL and the year parsed from the URL as shown in the examples. You will value accuracy over plausibility.\"),\n",
        "  few_shot_prompt,\n",
        "  (\"human\", \"{question}\"),\n",
        "])\n",
        "\n",
        "# mini = bigger than nano\n",
        "gpt_mini = ChatOpenAI(\n",
        "    model_name=\"gpt-4.1-mini\",\n",
        "    temperature=0.9,\n",
        "    openai_api_key=openai.api_key)\n",
        "\n",
        "chain_mini = full_prompt | gpt_mini"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4-4nvFyPQ9Bv"
      },
      "outputs": [],
      "source": [
        "def speaker_fuzzy_lookup(speaker, speaker_list):\n",
        "  # fuzzy lookup to get best match of speaker in list\n",
        "  best_match = process.extractOne(speaker, speaker_list)\n",
        "  return best_match[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qf_5SOEfbbUL"
      },
      "source": [
        "### **Augmented Generation**\n",
        "\n",
        "*  Generate a summary of the speaker's position on a topic with reference to their top k quotes.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SacBvjs4ievx"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "function that takes in the user's speaker and topic, finds the match in the speaker list.\n",
        "Then generates a summary of the speaker on a topic given the debates.\n",
        "'''\n",
        "def generate_answer(speaker_name, topic, list_of_speakers, num_results=5):\n",
        "\n",
        "    # the response to the user\n",
        "    response = \"\"\n",
        "\n",
        "    speaker_name = speaker_fuzzy_lookup(speaker_name, list_of_speakers)\n",
        "\n",
        "    # Retrieve relevant quotes\n",
        "    retrieved_text = search_speaker_position(speaker_name, topic, 5)\n",
        "    print(retrieved_text)\n",
        "\n",
        "    if \"No speeches found\" in retrieved_text or \"No relevant quotes found\" in retrieved_text:\n",
        "        return retrieved_text  # No results found, return directly\n",
        "\n",
        "    response_mini = chain_mini.invoke({\"question\": f\"Summarise {speaker_name}'s position on the topic: {topic}. Use the following quotes as reference: {retrieved_text}\", \"answer\": \"\"})\n",
        "\n",
        "    return response_mini.content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q-VDwJtzg_wi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a7fcbbf1-9ad4-4001-ba2e-f43fa9637748"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking for relevant utterences\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/root/.cache/chroma/onnx_models/all-MiniLM-L6-v2/onnx.tar.gz: 100%|██████████| 79.3M/79.3M [00:02<00:00, 33.3MiB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Done looking for relevant utterences\n",
            "{'ids': [['4a715fdc-d084-481b-9dd9-d9b32f9ef2d2', '6de3e6f4-6546-4917-bae9-cd5cb6905ed3', 'e1829429-1d8d-4156-8c63-cf64a16eeff4', '8a3f2bf8-228f-43d3-8fd1-f39165dcfc77', '5e1528e5-81da-4022-94be-3b76c935c16a']], 'embeddings': None, 'documents': None, 'uris': None, 'included': ['metadatas'], 'data': None, 'metadatas': [[{'text': 'The reason we have a housing crisis in this country-----', 'speaker': 'LeoVaradkar', 'url': 'https://data.oireachtas.ie/akn/ie/debateRecord/dail/2019-10-02/debate/mul@/main.xml'}, {'speaker': 'LeoVaradkar', 'url': 'https://data.oireachtas.ie/akn/ie/debateRecord/dail/2023-04-25/debate/mul@/main.xml', 'text': '-----or the Government for it. Both those analyses are far too simplistic. However, I have criticised people for objecting to housing developments. I will continue to do that. It is clear that we cannot fix the housing crisis without increased supply of all types of housing.'}, {'speaker': 'LeoVaradkar', 'url': 'https://data.oireachtas.ie/akn/ie/debateRecord/dail/2023-04-25/debate/mul@/main.xml', 'text': 'The causes of the housing crisis are multifactorial. I have gone through those factors on many occasions, but I have never blamed the Opposition for it-----'}, {'url': 'https://data.oireachtas.ie/akn/ie/debateRecord/dail/2023-03-29/debate/mul@/main.xml', 'text': '-----to come up with new and more dramatic language to describe the housing situation-----', 'speaker': 'LeoVaradkar'}, {'text': \"What I acknowledged yesterday is that the very real housing crisis we face in this country is a disaster for many people. It may not be for the 60% or 70% of us who are lucky enough to own our own homes, whether through a mortgage or outright, but it is certainly a disaster for people who cannot afford to buy a home and really want to and for people who are paying very significant proportions of their income in rent. I do not deny that is devastating for them and that there is a housing crisis, nor have I. I did not need to be told by anyone about the problems we face as a country. However, it is also the case, and it is just a fact, that the origins of this housing crisis lie in events that happened a very long time ago, when we had a property bubble that was followed by a banking collapse and a construction collapse. As a result of that, instead of building 30,000 or 35,000 new homes every year for a decade, which would have been the norm, virtually no houses were built for a very long time. We have a deficit of houses in the State. There are probably 250,000 fewer houses than we need. That is the origin of this housing crisis. The responsibility of those parties who are willing to be in government, and the Deputy's party has chosen not to enter government on several occasions, which is something we can come back to, is not to describe problems but to try to come up solutions and put them into action. That is hard. It is difficult work but it is work we are willing to do, unlike the Deputy's party. The Deputy asked what we are doing about it. The main thing we are doing is increasing supply. On its own, increasing supply will not solve the housing crisis but we will not solve it without increasing supply. Where are we at the moment? Approximately 25,000 new homes will be built this year, more than in any year for a very long time. Approximately 35,000 homes are now under construction, more than have been for a very long time. Approximately 45,000 have got planning permission in the past year so you can see a real pipeline of new housing coming on stream. The Deputy probably saw the figures that came out yesterday. In April of this year alone, more than 1,000 first-time buyers bought their first home. As far as I can remember, it has been a long time since 1,000 first-time buyers bought a new home in just one month. That is not enough. I want to see it increase to 2,000 and 3,000 a month. That is where we intend to be. Among the reasons that is happening is our help-to-buy scheme. More than 30,000 individuals and couples have received help in buying their first home through that scheme in recent years. That is something the Deputy's party wants to take away from them.\", 'url': 'https://data.oireachtas.ie/akn/ie/debateRecord/dail/2022-06-16/debate/mul@/main.xml', 'speaker': 'LeoVaradkar'}]], 'distances': None}\n",
            "\n",
            "### LeoVaradkar's Position on 'The housing crisis':\n",
            "\n",
            " **Quote 1 (debate url: https://data.oireachtas.ie/akn/ie/debateRecord/dail/2019-10-02/debate/mul@/main.xml):** The reason we have a housing crisis in this country-----...\n",
            "\n",
            " **Quote 2 (debate url: https://data.oireachtas.ie/akn/ie/debateRecord/dail/2023-04-25/debate/mul@/main.xml):** -----or the Government for it. Both those analyses are far too simplistic. However, I have criticised people for objecting to housing developments. I will continue to do that. It is clear that we cannot fix the housing crisis without increased supply of all types of housing....\n",
            "\n",
            " **Quote 3 (debate url: https://data.oireachtas.ie/akn/ie/debateRecord/dail/2023-04-25/debate/mul@/main.xml):** The causes of the housing crisis are multifactorial. I have gone through those factors on many occasions, but I have never blamed the Opposition for it-----...\n",
            "\n",
            " **Quote 4 (debate url: https://data.oireachtas.ie/akn/ie/debateRecord/dail/2023-03-29/debate/mul@/main.xml):** -----to come up with new and more dramatic language to describe the housing situation-----...\n",
            "\n",
            " **Quote 5 (debate url: https://data.oireachtas.ie/akn/ie/debateRecord/dail/2022-06-16/debate/mul@/main.xml):** What I acknowledged yesterday is that the very real housing crisis we face in this country is a disaster for many people. It may not be for the 60% or 70% of us who are lucky enough to own our own homes, whether through a mortgage or outright, but it is certainly a disaster for people who cannot afford to buy a home and really want to and for people who are paying very significant proportions of their income in rent. I do not deny that is devastating for them and that there is a housing crisis, ...\n",
            "\n",
            "Leo Varadkar acknowledges that 'the very real housing crisis we face in this country is a disaster for many people' (2022, https://data.oireachtas.ie/akn/ie/debateRecord/dail/2022-06-16/debate/mul@/main.xml). He emphasizes that 'the causes of the housing crisis are multifactorial' and rejects simplistic explanations or blaming the Opposition (2023, https://data.oireachtas.ie/akn/ie/debateRecord/dail/2023-04-25/debate/mul@/main.xml). Varadkar criticizes objections to housing developments, stating 'we cannot fix the housing crisis without increased supply of all types of housing' (2023, https://data.oireachtas.ie/akn/ie/debateRecord/dail/2023-04-25/debate/mul@/main.xml). Additionally, he notes efforts 'to come up with new and more dramatic language to describe the housing situation' (2023, https://data.oireachtas.ie/akn/ie/debateRecord/dail/2023-03-29/debate/mul@/main.xml).\n"
          ]
        }
      ],
      "source": [
        "# Example Usage\n",
        "speaker = \"LeoVaradkar\"\n",
        "topic = \"The housing crisis\"\n",
        "\n",
        "#read speakers.txt to get list of speakers\n",
        "with open(project_fpath+\"speakers.txt\", \"r\") as f:\n",
        "    SPEAKERS = f.read().splitlines()\n",
        "\n",
        "\n",
        "torch.cuda.empty_cache()\n",
        "final_answer = generate_answer(speaker, topic,  SPEAKERS)\n",
        "print(final_answer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OLxrHwUWA4Vk"
      },
      "source": [
        "# Web App"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WDFXSWTzA57U"
      },
      "outputs": [],
      "source": [
        "import gradio as gr\n",
        "\n",
        "def ask_about_speaker(speaker_name, topic):\n",
        "    answer = generate_answer(speaker_name, topic, SPEAKERS)\n",
        "    return answer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U9MAoh9pBFDd"
      },
      "outputs": [],
      "source": [
        "share=True\n",
        "iface = gr.Interface(\n",
        "    fn=ask_about_speaker,\n",
        "    inputs=[\n",
        "        gr.Textbox(label=\"Speaker Name\", placeholder=\"e.g., Ivana Bacik\"),\n",
        "        gr.Textbox(label=\"Topic\", placeholder=\"e.g., housing crisis\")\n",
        "    ],\n",
        "    outputs=gr.Textbox(label=\"Summary\"),\n",
        "    title=\"Speaker Position Summarizer\",\n",
        "    description=\"Ask what a public figure has said on any topic, and get a summary with direct quotes.\",\n",
        ")\n",
        "\n",
        "iface.launch()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dUyNbN7ItPk-"
      },
      "source": [
        "# PEFT - LoRA Fine-Tuning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DJrXpmprtVVh"
      },
      "source": [
        "\n",
        "\n",
        "*   Parameter Efficient Fine-Tuning\n",
        "*   Low Rank Adaption\n",
        "*   Fine-tune mistral-7b for this specific task.\n",
        "  * Freezes 7b weights and adds smaller weight matrix that projects to lower rank - small param size.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gs3NNVMhu_d_"
      },
      "source": [
        "## Freeze the Original Weights (W)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V-y3wpCttUTh"
      },
      "outputs": [],
      "source": [
        "for param in model.parameters():\n",
        "  param.requires_grad = False  # freeze the model - train adapters later\n",
        "  if param.ndim == 1:\n",
        "    # cast the small parameters (e.g. layernorm) to fp32 for stability\n",
        "    param.data = param.data.to(torch.float32)\n",
        "\n",
        "model.gradient_checkpointing_enable()  # reduce number of stored activations\n",
        "model.enable_input_require_grads()\n",
        "\n",
        "class CastOutputToFloat(nn.Sequential):\n",
        "    def forward(self, x): return super().forward(x).to(torch.float32)\n",
        "model.lm_head = CastOutputToFloat(model.lm_head)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "US_spfohvi8r"
      },
      "source": [
        "## Set up LoRA Adapters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FIl7INVSvlIu"
      },
      "outputs": [],
      "source": [
        "def print_trainable_parameters(model):\n",
        "    \"\"\"\n",
        "    Prints the number of trainable parameters in the model.\n",
        "    \"\"\"\n",
        "    trainable_params = 0\n",
        "    all_param = 0\n",
        "    for _, param in model.named_parameters():\n",
        "        all_param += param.numel()\n",
        "        if param.requires_grad: # count unfrozen params\n",
        "            trainable_params += param.numel()\n",
        "    print(\n",
        "        f\"trainable params: {trainable_params} || all params: {all_param} || trainable %: {100 * trainable_params / all_param}\"\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C1-mVpA2UHCV"
      },
      "outputs": [],
      "source": [
        "from peft import LoraConfig, get_peft_model\n",
        "\n",
        "config_LoRA = LoraConfig(\n",
        "    r=16, #attention heads\n",
        "    lora_alpha=32, #alpha scaling\n",
        "    # target_modules=[\"q_proj\", \"v_proj\"], #if you know the\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\" # set this for CLM or Seq2Seq\n",
        ")\n",
        "\n",
        "model_LoRA = get_peft_model(model, config_LoRA)\n",
        "print_trainable_parameters(model_LoRA)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6p2qzV-q3te-"
      },
      "source": [
        "## Example Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mZDX8nUfvqrV"
      },
      "outputs": [],
      "source": [
        "from datasets import Dataset\n",
        "import pandas as pd\n",
        "\n",
        "# list of example input output pairs, will use GPT-4o as standard.\n",
        "# use a symbol that won't have been in the original\n",
        "\n",
        "examples_df = pd.read_csv(project_fpath+\"Oireachtas_Examples.csv\")\n",
        "\n",
        "# Create a Hugging Face Dataset from the DataFrame\n",
        "train_dataset = Dataset.from_pandas(examples_df)\n",
        "\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "# Tokenize the input text and create input_ids and labels\n",
        "def preprocess_function(examples):\n",
        "    inputs = examples['X']\n",
        "    targets = examples['y']\n",
        "    model_inputs = tokenizer(inputs, truncation=True, padding=True)\n",
        "    # Setup the tokenizer for targets\n",
        "    with tokenizer.as_target_tokenizer():\n",
        "        labels = tokenizer(targets, truncation=True, padding=True)\n",
        "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
        "    return model_inputs\n",
        "\n",
        "# Apply the preprocessing function to the dataset\n",
        "tokenized_train_dataset = train_dataset.map(\n",
        "    preprocess_function,\n",
        "    batched=True,\n",
        "    remove_columns=train_dataset.column_names,\n",
        ")\n",
        "\n",
        "# Now, use tokenized_train_dataset in your Trainer\n",
        "trainer = transformers.Trainer(\n",
        "    model=model_LoRA,\n",
        "    train_dataset=tokenized_train_dataset,\n",
        "    args=transformers.TrainingArguments(\n",
        "        per_device_train_batch_size=1,\n",
        "        gradient_accumulation_steps=4,\n",
        "        warmup_steps=20,\n",
        "        max_steps=50,\n",
        "        learning_rate=2e-4,\n",
        "        fp16=True,\n",
        "        logging_steps=1,\n",
        "        output_dir='outputs'\n",
        "    ),\n",
        "    data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False)\n",
        ")\n",
        "model_LoRA.config.use_cache = False  # silence the warnings. Please re-enable for inference!\n",
        "trainer.train()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4",
      "toc_visible": true,
      "include_colab_link": true
    },
    "kaggle": {
      "accelerator": "nvidiaTeslaT4",
      "dataSources": [
        {
          "datasetId": 6616676,
          "sourceId": 10680757,
          "sourceType": "datasetVersion"
        }
      ],
      "dockerImageVersionId": 30840,
      "isGpuEnabled": true,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}